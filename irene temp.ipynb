{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDAUJSKpMmE7"
   },
   "source": [
    "# Data & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW_B2PyuMo_Z"
   },
   "source": [
    "Requirements:\n",
    "- Unigrams and bigrams\n",
    "- Folds 1-4 (training and tunning) <--- flexible\n",
    "- Folds 5 (validation) <-- flexible\n",
    "! Use cross-validation or (for random forests) out-of-bag evaluation to select the values of the hyperparameters of the algorithms on the training set.\n",
    "\n",
    "\n",
    "Deceptive = 1\n",
    "Truthful = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdPz0lX_Ru-y"
   },
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1760605030003,
     "user": {
      "displayName": "irene galimi",
      "userId": "11782113467320365577"
     },
     "user_tz": -120
    },
    "id": "ZTYl7CMvMlXF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def load_reviews_simple(base_path):\n",
    "    texts, labels, folds = [], [], []\n",
    "\n",
    "    for label_name, label_val in [(\"deceptive_from_MTurk\", 1),\n",
    "                                  (\"truthful_from_Web\", 0)]:\n",
    "        path = os.path.join(base_path, label_name)\n",
    "\n",
    "        for fold_name in sorted(os.listdir(path)):\n",
    "            fold_path = os.path.join(path, fold_name)\n",
    "            if not os.path.isdir(fold_path):\n",
    "                continue\n",
    "\n",
    "            for file in glob.glob(os.path.join(fold_path, \"*.txt\")):\n",
    "                with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                    texts.append(f.read().strip())\n",
    "                labels.append(label_val)\n",
    "                folds.append(fold_name)\n",
    "\n",
    "    return texts, labels, folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1760605031154,
     "user": {
      "displayName": "irene galimi",
      "userId": "11782113467320365577"
     },
     "user_tz": -120
    },
    "id": "ts4dtV9hPNXS",
    "outputId": "431b11d5-145f-40ad-a64b-4f4fb8fa4dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "{'fold1', 'fold4', 'fold5', 'fold2', 'fold3'}\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\irene\\Documents\\irene\\Università\\UU\\DM\\DM Assignment\\negative_polarity\"\n",
    "\n",
    "'''\n",
    "#base_path = \"/content/drive/MyDrive/Data Mining G25/Data/negative_polarity\" # Jagoda's path\n",
    "base_path = \"/content/drive/MyDrive/_AI_Master/DataMining/Data Mining G25/Data/negative_polarity\" # Sara's path\n",
    "'''\n",
    "texts, labels, folds = load_reviews_simple(base_path)\n",
    "\n",
    "print(len(texts))       # 800\n",
    "print(set(folds))       # {'fold1', ..., 'fold5'}\n",
    "print(labels[:10])      # [1, 1, 0, ...] -> should in it be all 1's?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1760605032398,
     "user": {
      "displayName": "irene galimi",
      "userId": "11782113467320365577"
     },
     "user_tz": -120
    },
    "id": "7MRvdDPfQo01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mcLw2cIRzYT"
   },
   "source": [
    "### Caching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1760605033687,
     "user": {
      "displayName": "irene galimi",
      "userId": "11782113467320365577"
     },
     "user_tz": -120
    },
    "id": "d8O26r0-Ra1N",
    "outputId": "ae3d29ed-1fc2-4e7b-9b6d-4c4ef9ca5c71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom joblib import dump\\ndump((texts, labels, folds), \"/content/drive/MyDrive/Data Mining G25/reviews.joblib\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from joblib import dump\n",
    "dump((texts, labels, folds), \"/content/drive/MyDrive/Data Mining G25/reviews.joblib\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 469,
     "status": "error",
     "timestamp": 1760605034694,
     "user": {
      "displayName": "irene galimi",
      "userId": "11782113467320365577"
     },
     "user_tz": -120
    },
    "id": "b4fOqJMgRchH",
    "outputId": "45630537-dcc4-44de-9f0d-cdf67db17d83"
   },
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "#path = \"/content/drive/MyDrive/Data Mining G25/reviews.joblib\" # Jagoda's path\n",
    "#path = \"/content/drive/MyDrive/_AI_Master/DataMining/Data Mining G25/reviews.joblib\" # Sara's path\n",
    "path = r\"C:\\Users\\irene\\Documents\\irene\\Università\\UU\\DM\\DM Assignment\\reviews.joblib\"\n",
    "\n",
    "\n",
    "texts, labels, folds = load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l10R1uw8WF2e"
   },
   "source": [
    "### Data Preprocessing: Vectorizations, Uni & Bigrams, Stratfied Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BwWu8Y0gWhHn"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def prepare_data(texts, labels, folds, ngram_range=(1,1), min_df=2, use_tfidf=True, cv_folds=5, random_state=42):\n",
    "    \"\"\"\n",
    "    train/test split with vectorization + StratifiedKFold CV splitter.\n",
    "\n",
    "    Args:\n",
    "        texts, labels, folds: dataset lists\n",
    "        ngram_range (tuple): (1,1)=unigrams, (1,2)=unigrams+bigrams\n",
    "        min_df: remove SPARSE terms >> when 2 only keeps words that appear in at least 2 reviews\n",
    "        use_tfidf: True=TF-IDF, weights based on the frequency of words in corpus\n",
    "        cv_folds (int): number of folds for cross-validation\n",
    "        random_state: 42\n",
    "\n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test, vectorizer, cv_splitter\n",
    "    \"\"\"\n",
    "    # split train/test (fold 1–4 vs fold 5)\n",
    "    train_texts = [t for t, f in zip(texts, folds) if f != \"fold5\"]\n",
    "    test_texts  = [t for t, f in zip(texts, folds) if f == \"fold5\"]\n",
    "\n",
    "    y_train = [y for y, f in zip(labels, folds) if f != \"fold5\"]\n",
    "    y_test  = [y for y, f in zip(labels, folds) if f == \"fold5\"]\n",
    "\n",
    "    # Vectorizer or CountVectorizer (Bag-of-Words) ??? >> JUST FOR BAYES MODEL :)))\n",
    "    VectorizerClass = TfidfVectorizer if use_tfidf else CountVectorizer\n",
    "    vectorizer = VectorizerClass(ngram_range=ngram_range, min_df=min_df)\n",
    "\n",
    "    X_train = vectorizer.fit_transform(train_texts) # fit only on training\n",
    "    X_test  = vectorizer.transform(test_texts) # apply to test set\n",
    "\n",
    "    # StratifiedKFolds >> Besides Randomr Forest, then we use OOB = True\n",
    "    cv_splitter = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state) # to use later in GridSearch\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, vectorizer, cv_splitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlyhVzKEb4qv"
   },
   "source": [
    "# Training Pipeline (NB, others TO ADD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11876,
     "status": "ok",
     "timestamp": 1760378815104,
     "user": {
      "displayName": "Jagoda Rutkowska",
      "userId": "01558359200133494356"
     },
     "user_tz": -120
    },
    "id": "BvAhJO71iclS",
    "outputId": "b38988eb-6f57-4208-e9bb-390aa9e55ca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: NAIVE_BAYES\n",
      "\n",
      ">>> ngram=(1, 1), min_df=2, folds=3\n",
      "\n",
      ">>> ngram=(1, 1), min_df=2, folds=5\n",
      "\n",
      ">>> ngram=(1, 1), min_df=2, folds=10\n",
      "\n",
      ">>> ngram=(1, 1), min_df=4, folds=3\n",
      "\n",
      ">>> ngram=(1, 1), min_df=4, folds=5\n",
      "\n",
      ">>> ngram=(1, 1), min_df=4, folds=10\n",
      "\n",
      ">>> ngram=(1, 1), min_df=6, folds=3\n",
      "\n",
      ">>> ngram=(1, 1), min_df=6, folds=5\n",
      "\n",
      ">>> ngram=(1, 1), min_df=6, folds=10\n",
      "\n",
      ">>> ngram=(1, 2), min_df=2, folds=3\n",
      "\n",
      ">>> ngram=(1, 2), min_df=2, folds=5\n",
      "\n",
      ">>> ngram=(1, 2), min_df=2, folds=10\n",
      "\n",
      ">>> ngram=(1, 2), min_df=4, folds=3\n",
      "\n",
      ">>> ngram=(1, 2), min_df=4, folds=5\n",
      "\n",
      ">>> ngram=(1, 2), min_df=4, folds=10\n",
      "\n",
      ">>> ngram=(1, 2), min_df=6, folds=3\n",
      "\n",
      ">>> ngram=(1, 2), min_df=6, folds=5\n",
      "\n",
      ">>> ngram=(1, 2), min_df=6, folds=10\n",
      "Saved NAIVE_BAYES overview to experiment_naive_bayes_overview.csv\n",
      "Saved misclassified samples to experiment_naive_bayes_misclassified.csv\n",
      "\n",
      "Top 3 results for naive_bayes\n",
      "  ngram_range  min_df  cv_folds   test_f1  test_acc\n",
      "0      (1, 2)       6        10  0.891720   0.89375\n",
      "1      (1, 2)       4         5  0.884615   0.88750\n",
      "2      (1, 2)       6         5  0.884615   0.88750\n",
      "MODEL: DECISION_TREE\n",
      "\n",
      ">>> ngram=(1, 1), min_df=2, folds=3\n",
      "\n",
      ">>> ngram=(1, 1), min_df=2, folds=5\n",
      "\n",
      ">>> ngram=(1, 1), min_df=2, folds=10\n",
      "\n",
      ">>> ngram=(1, 1), min_df=4, folds=3\n",
      "\n",
      ">>> ngram=(1, 1), min_df=4, folds=5\n",
      "\n",
      ">>> ngram=(1, 1), min_df=4, folds=10\n",
      "\n",
      ">>> ngram=(1, 1), min_df=6, folds=3\n",
      "\n",
      ">>> ngram=(1, 1), min_df=6, folds=5\n",
      "\n",
      ">>> ngram=(1, 1), min_df=6, folds=10\n",
      "\n",
      ">>> ngram=(1, 2), min_df=2, folds=3\n",
      "\n",
      ">>> ngram=(1, 2), min_df=2, folds=5\n",
      "\n",
      ">>> ngram=(1, 2), min_df=2, folds=10\n",
      "\n",
      ">>> ngram=(1, 2), min_df=4, folds=3\n",
      "\n",
      ">>> ngram=(1, 2), min_df=4, folds=5\n",
      "\n",
      ">>> ngram=(1, 2), min_df=4, folds=10\n",
      "\n",
      ">>> ngram=(1, 2), min_df=6, folds=3\n",
      "\n",
      ">>> ngram=(1, 2), min_df=6, folds=5\n",
      "\n",
      ">>> ngram=(1, 2), min_df=6, folds=10\n",
      "Saved DECISION_TREE overview to experiment_decision_tree_overview.csv\n",
      "Saved misclassified samples to experiment_decision_tree_misclassified.csv\n",
      "\n",
      "Top 3 results for decision_tree\n",
      "  ngram_range  min_df  cv_folds   test_f1  test_acc\n",
      "0      (1, 1)       4         5  0.649351    0.6625\n",
      "1      (1, 2)       2         3  0.628205    0.6375\n",
      "2      (1, 2)       2         5  0.628205    0.6375\n",
      "MODEL: GRADIENT_BOOSTING\n",
      "\n",
      ">>> ngram=(1, 1), min_df=2, folds=3\n",
      "\n",
      ">>> ngram=(1, 1), min_df=2, folds=5\n",
      "\n",
      ">>> ngram=(1, 1), min_df=2, folds=10\n",
      "\n",
      ">>> ngram=(1, 1), min_df=4, folds=3\n",
      "\n",
      ">>> ngram=(1, 1), min_df=4, folds=5\n",
      "\n",
      ">>> ngram=(1, 1), min_df=4, folds=10\n",
      "\n",
      ">>> ngram=(1, 1), min_df=6, folds=3\n",
      "\n",
      ">>> ngram=(1, 1), min_df=6, folds=5\n",
      "\n",
      ">>> ngram=(1, 1), min_df=6, folds=10\n",
      "\n",
      ">>> ngram=(1, 2), min_df=2, folds=3\n",
      "\n",
      ">>> ngram=(1, 2), min_df=2, folds=5\n",
      "\n",
      ">>> ngram=(1, 2), min_df=2, folds=10\n",
      "\n",
      ">>> ngram=(1, 2), min_df=4, folds=3\n",
      "\n",
      ">>> ngram=(1, 2), min_df=4, folds=5\n",
      "\n",
      ">>> ngram=(1, 2), min_df=4, folds=10\n",
      "\n",
      ">>> ngram=(1, 2), min_df=6, folds=3\n",
      "\n",
      ">>> ngram=(1, 2), min_df=6, folds=5\n",
      "\n",
      ">>> ngram=(1, 2), min_df=6, folds=10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_auc_score\n",
    ")\n",
    "\n",
    "\n",
    "# Training each model\n",
    "def train_model(model_name, X_train, y_train, cv, param_grid, scoring=\"f1\", n_jobs=-1):\n",
    "    \"\"\"Train a model using GridSearchCV OR OOB in case of Forest\"\"\"\n",
    "    if model_name == \"naive_bayes\":\n",
    "        model = MultinomialNB()\n",
    "        \n",
    "    elif model_name == \"decision_tree\":\n",
    "        model = DecisionTreeClassifier(random_state=42)\n",
    "        \n",
    "    elif model_name == \"gradient_boosting\":\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "    '''\n",
    "    #example\n",
    "    elif model_name == \"svm\":\n",
    "        model = LinearSVC()\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Model '{model_name}' not implemented yet.\")\n",
    "    '''\n",
    "    grid = GridSearchCV(model, param_grid=param_grid, cv=cv, scoring=scoring, n_jobs=n_jobs)\n",
    "    grid.fit(X_train, y_train)\n",
    "    return grid\n",
    "\n",
    "\n",
    "# Experiment Runnerrrr\n",
    "def run_experiments(\n",
    "    texts, labels, folds,\n",
    "    ngram_ranges=[(1,1), (1,2)],\n",
    "    min_dfs=[2,4,6],\n",
    "    use_tfidf=True,\n",
    "    cv_folds_list=[3,5,10],\n",
    "    random_state=42,\n",
    "    models_and_params=None,  # dict: {\"model\": param_grid} >> Later in the pipeline\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    save_prefix=\"experiment\"\n",
    "):\n",
    "    \"\"\"Run multiple models, each saved to its own CSV\n",
    "    (overview + misclassified sentences). ADD more hyperparamteres <3\"\"\"\n",
    "    # default models\n",
    "    if models_and_params is None:\n",
    "        models_and_params = {\n",
    "            \"naive_bayes\": {\"alpha\": [0.01, 0.1,0.2,0.3,0.4, 0.5, 1]},\n",
    "            \n",
    "            \"decision_tree\": {\n",
    "            \"criterion\": [\"gini\", \"entropy\"],\n",
    "            \"max_depth\": [None, 5, 10, 20],\n",
    "            \"ccp_alpha\": [0.0, 0.001, 0.01, 0.1],\n",
    "            \"min_samples_split\": [2, 5, 10],\n",
    "            \"min_samples_leaf\": [1, 2, 5]},\n",
    "            \n",
    "            \"gradient_boosting\": {\n",
    "            \"n_estimators\": [50, 100, 200],      # B: number of trees\n",
    "            \"learning_rate\": [0.01, 0.05, 0.1],  # λ: shrinkage\n",
    "            \"max_depth\": [2, 3, 5],             # fraction of samples per tree\n",
    "        }\n",
    "           # \"logreg\": {\"C\": [0.1, 1, 10]},\n",
    "           # \"svm\": {\"C\": [0.1, 1, 10]}\n",
    "        }\n",
    "\n",
    "    all_results_global = []\n",
    "    all_misclassified_global = []\n",
    "\n",
    "    # loop through models\n",
    "    for model_name, param_grid in models_and_params.items():\n",
    "        print(f\"MODEL: {model_name.upper()}\")\n",
    "\n",
    "        all_results = []\n",
    "        all_misclassified = []\n",
    "\n",
    "        for ngram_range in ngram_ranges:\n",
    "            for min_df in min_dfs:\n",
    "                for cv_folds in cv_folds_list:\n",
    "                    print(f\"\\n>>> ngram={ngram_range}, min_df={min_df}, folds={cv_folds}\")\n",
    "\n",
    "                    # prepare data\n",
    "                    X_train, y_train, X_test, y_test, vec, cv= prepare_data(\n",
    "                        texts, labels, folds,\n",
    "                        ngram_range=ngram_range,\n",
    "                        min_df=min_df,\n",
    "                        use_tfidf=use_tfidf,\n",
    "                        cv_folds=cv_folds,\n",
    "                        random_state=random_state\n",
    "                    )\n",
    "\n",
    "                    # train\n",
    "                    grid = train_model(model_name, X_train, y_train, cv, param_grid, scoring=scoring, n_jobs=n_jobs)\n",
    "                    best_model = grid.best_estimator_\n",
    "                    y_pred = best_model.predict(X_test)\n",
    "\n",
    "                    # metrics\n",
    "                    acc = accuracy_score(y_test, y_pred)\n",
    "                    prec = precision_score(y_test, y_pred)\n",
    "                    rec = recall_score(y_test, y_pred)\n",
    "                    f1 = f1_score(y_test, y_pred)\n",
    "                    f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "                    f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "                    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "                    try:\n",
    "                        if hasattr(best_model, \"predict_proba\"):\n",
    "                            y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "                            auc = roc_auc_score(y_test, y_proba)\n",
    "                        else:\n",
    "                            auc = np.nan\n",
    "                    except Exception:\n",
    "                        auc = np.nan\n",
    "\n",
    "\n",
    "                    cv_results = grid.cv_results_\n",
    "                    std_cv_f1 = cv_results[\"std_test_score\"][grid.best_index_]\n",
    "                    vocab_size = len(vec.get_feature_names_out())\n",
    "\n",
    "                    # misclassified sentences\n",
    "                    mis_idx = [i for i, (true, pred) in enumerate(zip(y_test, y_pred)) if true != pred]\n",
    "                    # Need the original test texts to save misclassified samples\n",
    "                    test_texts  = [t for t, f in zip(texts, folds) if f == \"fold5\"]\n",
    "                    for i in mis_idx:\n",
    "                        all_misclassified.append({\n",
    "                            \"model\": model_name,\n",
    "                            \"ngram_range\": str(ngram_range),\n",
    "                            \"min_df\": min_df,\n",
    "                            \"cv_folds\": cv_folds,\n",
    "                            \"true_label\": y_test[i],\n",
    "                            \"pred_label\": y_pred[i],\n",
    "                            \"text\": test_texts[i]\n",
    "                        })\n",
    "\n",
    "\n",
    "                    # collect results\n",
    "                    all_results.append({\n",
    "                        \"model\": model_name,\n",
    "                        \"ngram_range\": str(ngram_range),\n",
    "                        \"min_df\": min_df,\n",
    "                        \"cv_folds\": cv_folds,\n",
    "                        \"vocab_size\": vocab_size,\n",
    "                        \"best_params\": grid.best_params_,\n",
    "                        \"best_cv_f1_mean\": grid.best_score_,\n",
    "                        \"best_cv_f1_std\": std_cv_f1,\n",
    "                        \"test_acc\": acc,\n",
    "                        \"test_prec\": prec,\n",
    "                        \"test_rec\": rec,\n",
    "                        \"test_f1\": f1,\n",
    "                        \"test_f1_macro\": f1_macro,\n",
    "                        \"test_f1_weighted\": f1_weighted,\n",
    "                        \"TN\": tn, \"FP\": fp, \"FN\": fn, \"TP\": tp\n",
    "                    })\n",
    "\n",
    "        #Save files\n",
    "        df_results = pd.DataFrame(all_results).sort_values(by=\"test_f1\", ascending=False).reset_index(drop=True)\n",
    "        df_misclassified = pd.DataFrame(all_misclassified)\n",
    "\n",
    "        res_file = f\"{save_prefix}_{model_name}_overview.csv\"\n",
    "        mis_file = f\"{save_prefix}_{model_name}_misclassified.csv\"\n",
    "\n",
    "        df_results.to_csv(res_file, index=False)\n",
    "        df_misclassified.to_csv(mis_file, index=False)\n",
    "\n",
    "        print(f\"Saved {model_name.upper()} overview to {res_file}\")\n",
    "        print(f\"Saved misclassified samples to {mis_file}\")\n",
    "\n",
    "        all_results_global.extend(all_results)\n",
    "        all_misclassified_global.extend(all_misclassified)\n",
    "\n",
    "        #short summary\n",
    "        print(\"\\nTop 3 results for\", model_name)\n",
    "        print(df_results.head(3)[[\"ngram_range\", \"min_df\", \"cv_folds\", \"test_f1\", \"test_acc\"]])\n",
    "\n",
    "\n",
    "    return pd.DataFrame(all_results_global), pd.DataFrame(all_misclassified_global)\n",
    "\n",
    "\n",
    "df_all, df_all_mis = run_experiments(texts, labels, folds, models_and_params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of metrics to plot\n",
    "metrics = [\"test_acc\", \"test_prec\", \"test_rec\", \"test_f1\", \"test_f1_macro\", \"test_f1_weighted\"]\n",
    "\n",
    "# Loop through metrics and plot barplots\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(\n",
    "        data=df_all,\n",
    "        x=\"model\",\n",
    "        y=metric,\n",
    "        hue=\"ngram_range\",  # e.g., (1,1) for unigram, (1,2) for bigram\n",
    "        ci=\"sd\",            # show standard deviation as error bars\n",
    "        palette=\"Set2\"\n",
    "    )\n",
    "    plt.title(f\"{metric} by Model and N-gram Range\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"N-gram Range\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1BnRvMivhLRiAB3fyfdJ5eVfQBhV_kpOl",
     "timestamp": 1759580474057
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
